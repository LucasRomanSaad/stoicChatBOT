import chromadb from langchain_text_splitters import RecursiveCharacterTextSplitter from langchain_huggingface import HuggingFaceEmbeddings from langchain_community.document_loaders import PyPDFLoader import torch from langchain_groq import ChatGroq from langchain.prompts import PromptTemplate import os os.environ["GROQ_API_KEY"] = "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" # Initialize ChromaDB client = chromadb.PersistentClient(path="./research_db") collection = client.get_or_create_collection( name="ml_publications", metadata={"hnsw:space": "cosine"} ) # Set up our embedding model embeddings_model = HuggingFaceEmbeddings( model_name="sentence-transformers/all-MiniLM-L6-v2" ) def load_research_publications(documents_path): """Load a single research publication from a .pdf file and return as list of strings""" documents = [] try: loader = PyPDFLoader(documents_path) loaded_docs = loader.load() documents.extend(loaded_docs) print(f"Successfully loaded: {os.path.basename(documents_path)}") except Exception as e: print(f"Error loading {documents_path}: {str(e)}") print(f"\nTotal documents loaded: {len(documents)}") return documents def chunk_research_paper(paper_content, title): """Break a research paper into searchable chunks""" text_splitter = RecursiveCharacterTextSplitter( chunk_size=1000, # ~200 words per chunk chunk_overlap=200, # Overlap to preserve context separators=["\n\n", "\n", ". ", " ", ""] ) chunks = text_splitter.split_text(paper_content) # Add metadata to each chunk chunk_data = [] for i, chunk in enumerate(chunks): chunk_data.append({ "content": chunk, "title": title, "chunk_id": f"{title}_{i}", }) return chunk_data def embed_documents(documents: list[str]) -> list[list[float]]: """ Embed documents using a model. """ device = ( "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu" ) model = HuggingFaceEmbeddings( model_name="sentence-transformers/all-MiniLM-L6-v2", model_kwargs={"device": device}, ) # Directly embed the list of strings embeddings = model.embed_documents(documents) return embeddings def insert_publications(collection: chromadb.Collection, chunk_data: list[dict]): """ Insert documents into a ChromaDB collection. Args: collection (chromadb.Collection): The collection to insert documents into chunk_data (list[dict]): The chunk data with content, title, and chunk_id """ next_id = collection.count() # Extract content for embedding documents_content = [chunk["content"] for chunk in chunk_data] # Embed all documents at once for efficiency embeddings = embed_documents(documents_content) # Prepare metadata metadatas = [{"title": chunk["title"], "chunk_id": chunk["chunk_id"]} for chunk in chunk_data] # Generate IDs ids = [f"document_{next_id + i}" for i in range(len(chunk_data))] # Add to collection collection.add( embeddings=embeddings, ids=ids, documents=documents_content, metadatas=metadatas ) def search_research_db(query, collection, embeddings_model, top_k=5): """Find the most relevant research chunks for a query""" # Convert question to vector query_vector = embeddings_model.embed_query(query) # Search for similar content results = collection.query( query_embeddings=[query_vector], n_results=top_k, include=["documents", "metadatas", "distances"] ) # Format results relevant_chunks = [] for i, doc in enumerate(results["documents"][0]): relevant_chunks.append({ "content": doc, "title": results["metadatas"][0][i]["title"], "similarity": 1 - results["distances"][0][i] # Convert distance to similarity }) return relevant_chunks def answer_research_question(query, collection, embeddings_model, llm): """Generate an answer based on retrieved research""" # Get relevant research chunks relevant_chunks = search_research_db(query, collection, embeddings_model, top_k=3) # Build context from research context = "\n\n".join([ f"From {chunk['title']}:\n{chunk['content']}" for chunk in relevant_chunks ]) # Create research-focused prompt prompt_template = PromptTemplate( input_variables=["context", "question"], template=""" Based on the following research findings, answer the researcher's question: Research Context: {context} Researcher's Question: {question} Answer: Provide a comprehensive answer based on the research findings above. """ ) # Generate answer prompt = prompt_template.format(context=context, question=query) response = llm.invoke(prompt) return response.content, relevant_chunks # Main execution import glob print("Loading and splitting the text...") # List all PDF files except LucasRomanSaadCV.pdf pdf_files = [ f for f in glob.glob("*.pdf") ] chunk_data = [] for pdf_path in pdf_files: print(f"Processing {pdf_path}...") documents = load_research_publications(pdf_path) for i, doc in enumerate(documents): title = f"{os.path.splitext(os.path.basename(pdf_path))[0]}_doc{i+1}" chunks = chunk_research_paper(doc.page_content, title) chunk_data.extend(chunks) print("Embedding the chunks...") # This is now handled within insert_publications print("Inserting the chunks into the database...") # ChromaDB max batch size MAX_BATCH_SIZE = 5461 for start in range(0, len(chunk_data), MAX_BATCH_SIZE): end = start + MAX_BATCH_SIZE batch = chunk_data[start:end] print(f"Inserting batch {start} to {end}...") insert_publications(collection, batch) input_query = "What is a good stoic?" print("Searching for relevant chunks...") results = search_research_db(input_query, collection, embeddings_model) print(f"Found {len(results)} relevant chunks") try: llm = ChatGroq(model="llama3-70b-8192", temperature=0.1) print("Asking the question to the LLM...") answer, sources = answer_research_question(input_query, collection, embeddings_model, llm) print("\nAnswer:") print(answer) print("\nSources:") for source in sources: print(f"- {source['title']} (similarity: {source['similarity']:.3f})") except Exception as e: print(f"Error with Groq API: {e}") print("\nAvailable Groq models:") print("- llama3-8b-8192") print("- llama3-70b-8192") print("- mixtral-8x7b-32768") print("- gemma-7b-it") # Fallback: just show the search results print("\nSearch results (without LLM processing):") for i, result in enumerate(results): print(f"\n--- Result {i+1} ---") print(f"Title: {result['title']}") print(f"Similarity: {result['similarity']:.3f}") print(f"Content: {result['content'][:200]}...")